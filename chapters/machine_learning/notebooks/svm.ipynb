{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) originated from the statistical learning theory\n",
    "developed by Vapnik-Chervonenkis. As such, it represents a deep application of\n",
    "statistical theory that incorporates the VC dimension concepts we\n",
    "discussed in the first section. Let's start by looking at some pictures.\n",
    "Consider the two-dimensional classification problem shown in\n",
    "[Figure](#fig:svm_001).  [Figure](#fig:svm_001) shows two classes (gray and\n",
    "white\n",
    "circles) that can be separated by any of the lines shown. Specifically, any\n",
    "such separating line can be written as the locus of points ($\\mathbf{x}$) in\n",
    "the two-dimensional plane that satisfy the following,\n",
    "\n",
    "<!-- dom:FIGURE: [fig-machine_learning/svm_001.png, width=500 frac=0.45] In the\n",
    "two-dimensional plane, the two classes (gray and white circles) are easily\n",
    "separated by any one of the lines shown.   <div id=\"fig:svm_001\"></div>  -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:svm_001\"></div>\n",
    "\n",
    "<p>In the two-dimensional plane, the two classes (gray and white circles) are\n",
    "easily separated by any one of the lines shown.</p>\n",
    "<img src=\"fig-machine_learning/svm_001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "$$\n",
    "\\beta_0 + \\boldsymbol{\\beta}^T \\mathbf{x} = 0\n",
    "$$\n",
    "\n",
    " To classify  an arbitrary $\\mathbf{x}$ using this line, we just\n",
    "compute the sign of $\\beta_0+\\boldsymbol{\\beta}^T \\mathbf{x}$ and assign one\n",
    "class to the positive sign and the other class to the negative sign.  To\n",
    "uniquely specify such a separating line (or, hyperplane in a higher-dimensional\n",
    "space) we need additional criteria.\n",
    "\n",
    "\n",
    "[Figure](#fig:svm_002) shows the data with two bordering parallel lines that\n",
    "form a margin around the central separating line.  The *maximal margin\n",
    "algorithm* finds the widest margin and the unique separating line.  As a\n",
    "consequence, the algorithm uncovers the elements in the data that touch the\n",
    "margins. These are the *support* elements. The other elements\n",
    "away from the border are not relevent to the solution. This reduces\n",
    "model variance because the solution is insensitive to the removal of\n",
    "elements other than these supporting elements (usually a small minority).\n",
    "\n",
    "<!-- dom:FIGURE: [fig-machine_learning/svm_002.png, width=500 frac=0.55] The\n",
    "maximal margin algorithm finds the separating line that maximizes the margin\n",
    "shown. The elements that touch the margins are the support elements. The dotted\n",
    "elements are not relevent to the solution. <div id=\"fig:svm_002\"></div>  -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:svm_002\"></div>\n",
    "\n",
    "<p>The maximal margin algorithm finds the separating line that maximizes the\n",
    "margin shown. The elements that touch the margins are the support elements. The\n",
    "dotted elements are not relevent to the solution.</p>\n",
    "<img src=\"fig-machine_learning/svm_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "To see how this works for linearly separable classes, consider a\n",
    "training set consisting of $\\lbrace (\\mathbf{x},y) \\rbrace$ where\n",
    "$y\\in \\lbrace -1,1 \\rbrace$. For any point $\\mathbf{x}_i$, we\n",
    "compute the functional margin as $\\hat{ \\gamma_i }=y_i (\\beta_0 +\n",
    "\\boldsymbol{\\beta}^T \\mathbf{x}_i)$. Thus, $\\hat{\\gamma}_i >0$ when\n",
    "$\\mathbf{x}_i$ is correctly classified. The geometrical margin is\n",
    "$\\gamma = \\hat{\\gamma}/\\lVert\\boldsymbol{\\beta}\\rVert$. When\n",
    "$\\mathbf{x}_i$ is correctly classified, the geometrical margin is\n",
    "equal to the perpendicular distance from $\\mathbf{x}_i$ to the line.\n",
    "Let's look see how the maximal margin algorithm works.\n",
    "\n",
    "Let $M$ be the width of the margin.  The maximal margin algorithm is can be\n",
    "formulated as a quadratic programming problem. We want to simultaneously\n",
    "maximize the margin $M$ while ensuring that all of the data points are\n",
    "correctly classified.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\beta_0,\\boldsymbol{\\beta},\\lVert\\boldsymbol{\\beta}\\rVert=1}{\\text{m\n",
    "aximize}}\n",
    "& & M \\\\\\\n",
    "& \\text{subject to:}\n",
    "& & y_i(\\beta_0+\\boldsymbol{\\beta}^T \\mathbf{x}_i) \\geq M, \\; i = 1, \\ldots, N.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    " The first line says we want to generate a maximum value for $M$ by\n",
    "adjusting $\\beta_0$ and $\\boldsymbol{\\beta}$ while keeping\n",
    "$\\lVert\\boldsymbol{\\beta}\\rVert=1$. The functional margins for each $i^{th}$\n",
    "data element are the constraints to the problem and must be satisfied for every\n",
    "proposed solution. In words, the constraints enforce that the elements have to\n",
    "be correctly classified and outside of the margin around the separating line.\n",
    "With some reformulation, it turns out that\n",
    "$M=1/\\lVert\\boldsymbol{\\beta}\\rVert$ and this can be put into the following\n",
    "standard format,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\beta_0,\\boldsymbol{\\beta}}{\\text{minimize}}\n",
    "& & \\lVert\\boldsymbol{\\beta}\\rVert \\\\\\\n",
    "& \\text{subject to:}\n",
    "& & y_i(\\beta_0+\\boldsymbol{\\beta}^T \\mathbf{x}_i) \\geq 1, \\; i = 1, \\ldots, N.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    " This is a convex optimization problem  and can be solved using\n",
    "powerful\n",
    "methods in that area.\n",
    "\n",
    "The situation becomes more complex when the two classes are not separable and\n",
    "we have to allow some unavoidable mixing between the two classes in the\n",
    "solution. This means that the contraints have to modified as in the following,\n",
    "\n",
    "$$\n",
    "y_i(\\beta_0+\\boldsymbol{\\beta}^T \\mathbf{x}_i) \\geq M(1-\\xi_i)\n",
    "$$\n",
    "\n",
    " where the $\\xi_i$ are the slack variables and represent the\n",
    "proportional amount tha the prediction is on the wrong side of the margin. Thus,\n",
    "elements are misclassified when $\\xi_i>1$. With these additional variables,\n",
    "we have a more general formulation of the convex optimization problem,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\underset{\\beta_0,\\boldsymbol{\\beta}}{\\text{minimize}}\n",
    "& & \\lVert\\boldsymbol{\\beta}\\rVert \\\\\\\n",
    "& \\text{subject to:}\n",
    "& & y_i(\\beta_0+\\boldsymbol{\\beta}^T \\mathbf{x}_i) \\geq 1-\\xi_i, \\\\\\\n",
    "& & & \\xi_i \\geq 0, \\sum \\xi_i \\leq \\texttt{constant}, \\; i = 1, \\ldots, N.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    " which can be rewritten in the following equivalent form,\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:svm\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\underset{\\beta_0,\\boldsymbol{\\beta}}{\\text{minimize}}\n",
    "& & \\frac{1}{2}\\lVert\\boldsymbol{\\beta}\\rVert + C \\sum \\xi_i \\\\\\\n",
    "& \\text{subject to:}\n",
    "& & y_i(\\beta_0+\\boldsymbol{\\beta}^T \\mathbf{x}_i) \\geq 1-\\xi_i, \\xi_i \\geq 0 \\;\n",
    "i = 1, \\ldots, N.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\\label{eq:svm} \\tag{1}\n",
    "$$\n",
    "\n",
    " Because the $\\xi_i$ terms are all positive, the objective\n",
    "is to maximize the margin (i.e., minimize $\\lVert\\boldsymbol{\\beta}\\rVert$)\n",
    "while minimizing the proportional drift of the predictions to the wrong side\n",
    "of the margin (i.e., $C \\sum \\xi_i$). Thus, large values of $C$ shunt\n",
    "algorithmic focus towards the correctly classified points near the\n",
    "decision boundary and small values focus on further data. The value $C$ is\n",
    "a hyperparameter for the SVM.\n",
    "\n",
    "The good news is that all of these complicated pieces are handled neatly inside\n",
    "of Scikit-learn. The following sets up the linear *kernel* for the SVM (more on\n",
    "kernels soon),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.svm import SVC\n",
    "sv = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create some synthetic data using `make_blobs` and then\n",
    "fit it to the SVM,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "X,y=make_blobs(n_samples=200, centers=2, n_features=2,\n",
    "               random_state=0,cluster_std=.5)\n",
    "sv.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, the SVM now has the estimated support vectors and the\n",
    "coefficients of the $\\boldsymbol{\\beta}$  in the `sv.support_vectors_` and\n",
    "`sv.coef_` attributes, respectively. [Figure](#fig:svm_003) shows the two\n",
    "sample classes (white and gray circles) and the line separating them that was\n",
    "found by the maximal margin algorithm. The two parallel dotted lines show  the\n",
    "margin. The large circles enclose the support vectors, which are the data\n",
    "elements that are relevent to the solution. Notice that only these elements\n",
    "can touch the edges of the margins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import subplots\n",
    "import numpy as np\n",
    "xi = np.linspace(X[:,0].min(),X[:,0].max(),100)\n",
    "\n",
    "fig,ax=subplots()\n",
    "_=ax.scatter(X[:,0],X[:,1],c=y,s=50,cmap='gray',marker='o',alpha=.3)\n",
    "_=ax.plot(sv.support_vectors_[:,0],sv.support_vectors_[:,1],'ko',markersize=20,alpha=.2)\n",
    "_=ax.plot(xi,-sv.coef_[0,0]/sv.coef_[0,1]*xi- sv.intercept_/sv.coef_[0,1],'k',lw=3.)\n",
    "margin = np.linalg.norm(sv.coef_)\n",
    "_=ax.plot(xi,-sv.coef_[0,0]/sv.coef_[0,1]*xi-(sv.intercept_+margin/2.)/sv.coef_[0,1],'--k',lw=3.)\n",
    "_=ax.plot(xi,-sv.coef_[0,0]/sv.coef_[0,1]*xi-(sv.intercept_-margin/2.)/sv.coef_[0,1],'--k',lw=3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/svm_003.png, width=500 frac=0.75]  The\n",
    "two class shown (white and gray circles) are linearly separable. The maximal\n",
    "margin solution is shown by the dark black line in the middle. The dotted lines\n",
    "show the extent of the margin.  The large circles indicate the support vectors\n",
    "for the maximal margin solution. <div id=\"fig:svm_003\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:svm_003\"></div>\n",
    "\n",
    "<p>The two class shown (white and gray circles) are linearly separable. The\n",
    "maximal margin solution is shown by the dark black line in the middle. The\n",
    "dotted lines show the extent of the margin.  The large circles indicate the\n",
    "support vectors for the maximal margin solution.</p>\n",
    "<img src=\"fig-machine_learning/svm_003.png\" width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def draw_margins(sv,X,y,ax=None):\n",
    "    sv.fit(X,y)\n",
    "    xi = np.linspace(X[:,0].min(),X[:,0].max(),100)\n",
    "    if ax is None: fig,ax=subplots()\n",
    "    _=ax.scatter(X[:,0],X[:,1],c=y,s=50,cmap='gray',marker='o',alpha=.3)\n",
    "    _=ax.plot(sv.support_vectors_[:,0],sv.support_vectors_[:,1],'ko',markersize=20,alpha=.2)\n",
    "    _=ax.plot(xi,-sv.coef_[0,0]/sv.coef_[0,1]*xi- sv.intercept_/sv.coef_[0,1],'k',lw=3.)\n",
    "    margin = np.linalg.norm(sv.coef_)\n",
    "    _=ax.plot(xi,-sv.coef_[0,0]/sv.coef_[0,1]*xi- (sv.intercept_+margin/2.)/sv.coef_[0,1],'--k',lw=3.)\n",
    "    _=ax.plot(xi,-sv.coef_[0,0]/sv.coef_[0,1]*xi- (sv.intercept_-margin/2.)/sv.coef_[0,1],'--k',lw=3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X, y = make_blobs(n_samples=50, centers=2, n_features=2,\n",
    "                  cluster_std=1,random_state=0)\n",
    "\n",
    "fig,axs = subplots(2,2,sharex=True,sharey=True)\n",
    "#fig.set_size_inches((12,6))\n",
    "sv = SVC(kernel='linear',C=.0100)\n",
    "draw_margins(sv,X,y,ax=axs[0,0])\n",
    "_=axs[0,0].set_title('C=0.01')\n",
    "sv = SVC(kernel='linear',C=1)\n",
    "draw_margins(sv,X,y,ax=axs[0,1])\n",
    "_=axs[0,1].set_title('C=1')\n",
    "sv = SVC(kernel='linear',C=100)\n",
    "draw_margins(sv,X,y,ax=axs[1,0])\n",
    "_=axs[1,0].set_title('C=100')\n",
    "sv = SVC(kernel='linear',C=10000)\n",
    "draw_margins(sv,X,y,ax=axs[1,1])\n",
    "_=axs[1,1].set_title('C=10000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Figure](#fig:svm_004) shows what happens when the value of $C$ changes.\n",
    "Increasing this value emphasizes the $\\xi$ part of the objective function in\n",
    "Equation [eq:svm](#eq:svm). As shown in the top left panel, a small value for\n",
    "$C$ means that\n",
    "the algorithm is willing to accept many support vectors at the expense of\n",
    "maximizing the margin. That is, the proportional amount that predictions are on\n",
    "the wrong side of the margin is more acceptable with smaller $C$.  As the value\n",
    "of $C$ increases, there are fewer support vectors because the optimization\n",
    "process prefers to eliminate support vectors that are far away from the margins\n",
    "and accept fewer of these that encroach into the margin. Note that as the value\n",
    "of $C$ progresses through this figure, the separating line tilts slightly.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-machine_learning/svm_004.png, width=500 frac=0.95] The\n",
    "maximal margin algorithm finds the separating line that maximizes the margin\n",
    "shown. The elements that touch the margins are the support elements. The dotted\n",
    "elements are not relevent to the solution. <div id=\"fig:svm_004\"></div>  -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:svm_004\"></div>\n",
    "\n",
    "<p>The maximal margin algorithm finds the separating line that maximizes the\n",
    "margin shown. The elements that touch the margins are the support elements. The\n",
    "dotted elements are not relevent to the solution.</p>\n",
    "<img src=\"fig-machine_learning/svm_004.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "## Kernel Tricks\n",
    "\n",
    "Support Vector Machines provide a powerful method to deal with linear\n",
    "separations, but they can also apply to non-linear boundaries by\n",
    "exploiting the so-called *kernel trick*.  The convex optimization\n",
    "formulation of the SVM includes a *dual* formulation that leads to a\n",
    "solution that requires only the inner-products of the features. The\n",
    "kernel trick is to substitute inner-products by nonlinear kernel\n",
    "functions.  This can be thought of as mapping the original features\n",
    "onto a possibly infinite dimensional space of new features.  That is,\n",
    "if the data are not linearly separable in two-dimensional space (for\n",
    "example) maybe they are separable in three-dimensional space (or\n",
    "higher)?\n",
    "\n",
    "To make this concrete, suppose the original input space is\n",
    "$\\mathbb{R}^n$ and we want to use a non-linear mapping\n",
    "$\\psi:\\mathbf{x} \\mapsto \\mathcal{F}$ where $\\mathcal{F}$ is an\n",
    "inner-product space of higher dimension.  The kernel trick is to\n",
    "calculate the inner-product in $\\mathcal{F}$ using a kernel\n",
    "function, $K(\\mathbf{x}_i,\\mathbf{x}_j) = \\langle\n",
    "\\psi(\\mathbf{x}_i),\\psi(\\mathbf{x}_j)\\rangle$. The long way to\n",
    "compute this is to first compute $\\psi(\\mathbf{x})$ and then do the\n",
    "inner-product. The kernel-trick way to do it is to use the kernel\n",
    "function and avoid computing $\\psi$. In other words, the kernel\n",
    "function returns what the inner-product in $\\mathcal{F}$ would have\n",
    "returned if $\\psi$ had been applied. For example, to achieve an\n",
    "$n^{th}$ polynomial mapping of the input space, we can use\n",
    "$\\kappa(\\mathbf{x}_i,\\mathbf{x}_j)=(\\mathbf{x}_i^T\\mathbf{x}_j+\\theta)^n$.\n",
    "For example, suppose the input space is $\\mathbb{R}^2$ and\n",
    "$\\mathcal{F}=\\mathbb{R}^4$ and we have the following mapping,\n",
    "\n",
    "$$\n",
    "\\psi(\\mathbf{x}) : (x_0,x_1) \\mapsto (x_0^2,x_1^2,x_0 x_1, x_1 x_0)\n",
    "$$\n",
    "\n",
    " The inner product in $\\mathcal{F}$ is then,\n",
    "\n",
    "$$\n",
    "\\langle \\psi(\\mathbf{x}),\\psi(\\mathbf{y})  \\rangle = \\langle\n",
    "\\mathbf{x},\\mathbf{y}  \\rangle^2\n",
    "$$\n",
    "\n",
    " In other words, the kernel is the square of the inner\n",
    "product in input space. The advantage of using the kernel instead of\n",
    "simply enlarging the feature space is computational because you only\n",
    "need to compute the kernel on all distinct pairs of the input space.\n",
    "The following example should help make this concrete. First we create\n",
    "some Sympy variables,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "import sympy as S\n",
    "x0,x1=S.symbols('x:2',real=True)\n",
    "y0,y1=S.symbols('y:2',real=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the $\\psi$ function that maps into $\\mathbb{R}^4$\n",
    "and the corresponding kernel function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "psi = lambda x,y: (x**2,y**2,x*y,x*y)\n",
    "kern = lambda x,y: S.Matrix(x).dot(y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the inner product in $\\mathbb{R}^4$  is\n",
    "equal to the kernel function, which only uses wthe $\\mathbb{R}^2$\n",
    "variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "print(S.Matrix(psi(x0,x1)).dot(psi(y0,y1)))\n",
    "print(S.expand(kern((x0,x1),(y0,y1))) # same as above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Regression Using Kernels.**  Recall our favorite\n",
    "linear regression problem from the regularization chapter,\n",
    "\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\beta}}  \\Vert y - \\mathbf{X}\\boldsymbol{\\beta}\\Vert^2\n",
    "$$\n",
    "\n",
    " where $\\mathbf{X}$ is a $n\\times m$ matrix with $m>n$. As\n",
    "we discussed, there are multiple solutions to this problem. The\n",
    "least-squares solution is the following:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\beta}_{LS}=\\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)^{\\text{-1}}\\mathbf{\n",
    "y}\n",
    "$$\n",
    "\n",
    " Given a new feature vector $\\mathbf{x}$, the corresponding estimator\n",
    "for $\\mathbf{y}$ is the following,\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = \\mathbf{x}^T\\boldsymbol{\\beta}_{LS}=\\mathbf{x}^T\\mathbf{X}^T(\n",
    "\\mathbf{X}\\mathbf{X}^T)^{\\text{-1}}\\mathbf{y}\n",
    "$$\n",
    "\n",
    " Using the kernel trick, the solution can be written more generally as\n",
    "the following,\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}}=\\mathbf{k}(\\mathbf{x})^T\\mathbf{K}^{\\text{-1}}\\mathbf{y}\n",
    "$$\n",
    "\n",
    " where the $n\\times n$ kernel matrix $\\mathbf{K}$ replaces\n",
    "$\\mathbf{X}\\mathbf{X}^T$ and where $\\mathbf{k}(\\mathbf{x})$ is a $n$-vector of\n",
    "components $\\mathbf{k}(\\mathbf{x})=[\\kappa(\\mathbf{x}_i,\\mathbf{x})]$ and where\n",
    "$\\mathbf{K}_{i,j}=\\kappa(\\mathbf{x}_i,\\mathbf{x}_j)$ for the kernel function\n",
    "$\\kappa$.  With this more general setup, we can substitute\n",
    "$\\kappa(\\mathbf{x}_i,\\mathbf{x}_j)=(\\mathbf{x}_i^T\\mathbf{x}_j+\\theta)^n$ for\n",
    "$n^{th}$-order polynomial regression [[bauckhagenumpy]](#bauckhagenumpy). Note\n",
    "that ridge\n",
    "regression can also be incorporated by inverting $(\\mathbf{K}+\\alpha\n",
    "\\mathbf{I})$, which can help stabilize poorly conditioned $\\mathbf{K}$ matrices\n",
    "with a tunable $\\alpha$ hyper-parameter [[bauckhagenumpy]](#bauckhagenumpy).\n",
    "\n",
    "For some kernels, the enlarged $\\mathcal{F}$ space is infinite-dimensional.\n",
    "Mercer's conditions provide technical restrictions on the kernel functions.\n",
    "Powerful, well-studied kernels have been implemented in Scikit-learn. The\n",
    "advantage of kernel functions may evaporate for when $n\\rightarrow m$ in which\n",
    "case using the $\\psi$ functions instead can be more practicable.\n",
    "\n",
    "<!-- !bt -->\n",
    "<!-- \\begin{pyconsole} -->\n",
    "<!-- sv = SVC(kernel='rbf',C=1000) -->\n",
    "<!-- sv.fit(X,y) -->\n",
    "<!-- \\end{pyconsole} -->\n",
    "<!-- !et -->\n",
    "\n",
    "<!-- FIGURE: [fig-machine_learning/svm_005.png, width=500 frac=0.85] Using a\n",
    "radial basis function kernel, the SVM can generate a curved separating surface\n",
    "that can classify the two classes shown. <div id=\"fig:svm_005\"></div> -->\n",
    "\n",
    "<!-- As shown in [Figure](#fig:svm_002), the maximal margin algorithm finds the\n",
    "-->\n",
    "<!-- separating line that maximizes the margin shown. As a result, the data\n",
    "shown by -->\n",
    "<!-- the dotted circles are no longer relevant to the *support* of the line.\n",
    "That -->\n",
    "<!-- is, the dotted circles could be removed with changing the final result. -->\n",
    "\n",
    "<!-- Kernel trick -->\n",
    "<!-- objective function includes VC dimension -->\n",
    "\n",
    "<!-- *Modern Multivariate Statistical Techniques Izenman, p. 371* -->\n",
    "<!-- *Learning and Soft computing by Kecman, p.154, 171, 186* -->\n",
    "<!-- *Mastering machine learning with Scikit-learn, p.174* -->\n",
    "<!-- *Gaussian Processes for Machine Learning, p. 163* -->\n",
    "<!-- *Elements of statistical learning p.418* -->\n",
    "<!-- *Kernel methods pattern Taylor p.43* -->\n",
    "<!-- *Learning with Kernels, p.43* -->\n",
    "<!-- *An Intro to Machine Learning by james, p.362* -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.pylab import cm\n",
    "xi = np.linspace(X[:,0].min(),X[:,0].max(),100)\n",
    "yi = np.linspace(X[:,1].min(),X[:,1].max(),100)\n",
    "\n",
    "fig,ax=subplots()\n",
    "_=ax.scatter(X[:,0],X[:,1],c=y,s=50,cmap='gray',marker='o',alpha=.3)\n",
    "Xi,Yi = np.meshgrid(xi,yi)\n",
    "Zi=sv.predict(np.c_[Xi.ravel(),Yi.ravel()]).reshape(Xi.shape)\n",
    "\n",
    "_=ax.contourf(Xi,Yi,Zi,cmap=cm.Paired,alpha=0.2);"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
