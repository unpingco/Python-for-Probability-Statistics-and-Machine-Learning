{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from __future__ import division\n",
    "import sys\n",
    "old_displayhook=sys.displayhook\n",
    "def displ(x):\n",
    "   if x is None: return\n",
    "   pprint(x,width=40,depth=1)\n",
    "\n",
    "sys.displayhook=displ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing so practical as a good theory. In this section, we establish\n",
    "the formal framework for thinking about machine learning.  This framework will\n",
    "help us think beyond particular methods for machine learning so we can\n",
    "integrate new methods or combine existing methods intelligently.\n",
    "\n",
    "Both machine learning and statistics share the common goal of trying to derive\n",
    "understanding from data.  Some historical perspective helps. Most of the\n",
    "methods in statistics were derived towards the start of the 20th century when\n",
    "data were hard to come by. Society was preoccupied with the potential dangers\n",
    "of human overpopulation and work was focused on studying agriculture and\n",
    "crop yields. At this time, even a dozen data points was considered\n",
    "plenty.  Around the same time, the deep foundations of probability were being\n",
    "established by Kolmogorov. Thus, the lack of data meant that the conclusions\n",
    "had to be buttressed by strong assumptions and solid  mathematics provided by\n",
    "the emerging theory of probability.  Furthermore, inexpensive powerful\n",
    "computers were not yet widely available. The situation today is much different:\n",
    "there are lots of data collected and powerful and easily programmable computers\n",
    "are available. The important problems no longer revolve around a dozen data\n",
    "points on a farm acre, but rather millions of points on a square millimeter of\n",
    "a DNA microarray.  Does this mean that statistics will be superseded by machine\n",
    "learning?\n",
    "\n",
    "In contrast to classical statistics, which is concerned with developing models\n",
    "that characterize, explain, and describe phenomena, machine learning is\n",
    "primarily concerned with prediction, usually at the expense of all else.  Areas\n",
    "like exploratory statistics are very closely related to machine learning, but\n",
    "the degree of emphasis on prediction is still distinguishing.  In some sense,\n",
    "this is unavoidable due the size of the data machine learning can reduce.  In\n",
    "other words, machine learning can help distill a table of a million columns\n",
    "into one hundred columns, but can we still interpret one hundred columns\n",
    "meaningfully? In classical statistics,  this was never an issue because data\n",
    "were of a much smaller scale.   Whereas mathematical models, usually normal\n",
    "distributions, fitted with observations are common in statistics, machine\n",
    "learning uses data to construct models that sit on complicated data structures\n",
    "and exploit nonlinear optimizations that lack closed-form solutions. A common\n",
    "maxim is that statistics is data plus analytical theory and machine learning is\n",
    "data plus computable structures. This makes it seem like machine learning is\n",
    "completely ad-hoc and devoid of underlying theory, but this is not the case,\n",
    "and both machine learning and statistics share many important theoretical\n",
    "results. By way of contrast, let us consider a concrete problem.\n",
    "\n",
    "Let's consider the classic balls in urns problem (see\n",
    "[Figure](#fig:learning_theory_tmp_001)): we have an urn containing red and\n",
    "blue balls and we draw five balls from the urn, note the color of each\n",
    "ball, and then try to determine the proportion of red and blue balls\n",
    "in the urn. We have already studied many statistical methods for\n",
    "dealing with this problem in previous sections. Now, let's generalize\n",
    "the problem slightly.  Suppose the urn is filled with white balls and\n",
    "there is some target unknown function $f$ that paints each selected\n",
    "ball either red or blue (see [Figure](#fig:learning_theory_tmp_002)).\n",
    "The machine learning problem is how to find the $f$ function, given\n",
    "only the observed red/blue balls. So far, this doesn't sound much\n",
    "different from the statistical problem. However, now we want to take\n",
    "our estimated $f$ function, say, $\\hat{f}$, and use it to predict the\n",
    "next handful of balls from another urn.  Now, here's where the story\n",
    "takes a sharp turn. Suppose the next urn *already* has some red and\n",
    "blue balls in it?  Then, applying the function $f$ may result in\n",
    "purple balls which were not seen in the *training* data (see\n",
    "[Figure](#fig:learning_theory_tmp_003)). Now, what can we do?  We have just\n",
    "scraped the surface of the issues machine learning must confront using\n",
    "methods that are not part of the statistics canon.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-machine_learning/learning_theory_tmp_001.png, width=500\n",
    "frac=0.50] In the classical statistics problem, we observe a sample and model\n",
    "what the urn contains. <div id=\"fig:learning_theory_tmp_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:learning_theory_tmp_001\"></div>\n",
    "\n",
    "<p>In the classical statistics problem, we observe a sample and model what the\n",
    "urn contains.</p>\n",
    "<img src=\"fig-machine_learning/learning_theory_tmp_001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig-machine_learning/learning_theory_tmp_002.png, width=500\n",
    "frac=0.60] In the machine learning problem, we want the function that colors the\n",
    "marbles. <div id=\"fig:learning_theory_tmp_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:learning_theory_tmp_002\"></div>\n",
    "\n",
    "<p>In the machine learning problem, we want the function that colors the\n",
    "marbles.</p>\n",
    "<img src=\"fig-machine_learning/learning_theory_tmp_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig-machine_learning/learning_theory_tmp_003.png, width=500\n",
    "frac=0.60] The problem is further complicated because we may see colored marbles\n",
    "that were not present in the original problem.  <div\n",
    "id=\"fig:learning_theory_tmp_003\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:learning_theory_tmp_003\"></div>\n",
    "\n",
    "<p>The problem is further complicated because we may see colored marbles that\n",
    "were not present in the original problem.</p>\n",
    "<img src=\"fig-machine_learning/learning_theory_tmp_003.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "## Introduction to Theory of Machine Learning\n",
    "\n",
    "Some formality and an example can get us going. We define the unknown\n",
    "target function, $f:\\mathcal{X} \\mapsto \\mathcal{Y}$. The training set\n",
    "is $\\left\\{(x,y)\\right\\}$ which means that we only see the function's\n",
    "inputs/outputs. The hypothesis set $\\mathcal{H}$ is the set of all\n",
    "possible guesses at $f$. This is the set from which we will ultimately\n",
    "draw our final estimate, $\\hat{f}$. The machine learning problem is\n",
    "how to  derive the best element from the hypothesis set by using the\n",
    "training set.  Let's consider a concrete example in the code below.\n",
    "Suppose $\\mathcal{X}$ consists of all three-bit vectors (i.e.,\n",
    "$\\mathcal{X}=\\left\\{000,001,\\ldots,111\\right\\}$) as in the code below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "df=DataFrame(index=pd.Index(['{0:04b}'.format(i) for i in range(2**4)],\n",
    "                            dtype='str',\n",
    "                            name='x'),columns=['f'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The string specification above uses Python's advanced string\n",
    "formatting mini-language. In this case, the specification says to\n",
    "convert the integer into a fixed-width, four-character (`04b`) binary\n",
    "representation.\n",
    "\n",
    "\n",
    "\n",
    " Next, we define the target function $f$ below which just\n",
    "checks if the number of zeros in the binary representation exceeds the\n",
    "number of ones.  If so, then the function outputs `1` and `0`\n",
    "otherwise (i.e., $\\mathcal{Y}=\\left\\{0,1\\right\\}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "df.f=np.array(df.index.map(lambda i:i.count('0')) \n",
    "               > df.index.map(lambda i:i.count('1')),dtype=int)\n",
    "df.head(8) # show top half only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypothesis set for this problem is the set of *all*\n",
    "possible functions of $\\mathcal{X}$. The set $\\mathcal{D}$ represents\n",
    "all possible input/output pairs. The corresponding hypothesis set\n",
    "$\\mathcal{H}$ has $2^{16}$ elements, one of which matches $f$. There are\n",
    "$2^{16}$ elements in the hypothesis set because for each of sixteen\n",
    "input elements, there are two possible corresponding values zero or\n",
    "one for each input. Thus, the size of the hypothesis set is $2\\times\n",
    "2\\times\\ldots\\times 2=2^{16}$.  Now, presented\n",
    "with a training set consisting of the first eight input/output pairs,\n",
    "our goal is to minimize errors over the training set\n",
    "($E_{\\texttt{in}}(\\hat{f})$).  There are $2^8$ elements from the\n",
    "hypothesis set that exactly match $f$ over the training set.  But how\n",
    "to pick among these $2^8$ elements?  It seems that we are stuck here.\n",
    "We need another element from the problem in order to proceed. The\n",
    "extra piece we need is to assume that the training set represents a\n",
    "random sampling (*in-sample* data) from a greater population\n",
    "(*out-of-sample* data) that would be consistent with the population\n",
    "that $\\hat{f}$ would ultimately predict upon. In other words, we are\n",
    "assuming a stable probability structure for both the in-sample and\n",
    "out-of-sample data. This is a major assumption!\n",
    "\n",
    "\n",
    "There is a subtle consequence of this assumption --- whatever the\n",
    "machine learning method does once deployed, in order for it to\n",
    "continue to work, it cannot disturb the data environment that it was\n",
    "trained on. Said differently, if the method is not to be trained\n",
    "continuously, then it cannot break this assumption by altering the\n",
    "generative environment that produced the data it was trained on.  For\n",
    "example, suppose we develop a model that predicts hospital\n",
    "readmissions based on seasonal weather and patient health.  Because\n",
    "the model is so effective, in the next six months, the hospital\n",
    "forestalls readmissions by delivering\n",
    "interventions that improve patient health.  Clearly using\n",
    "the model cannot change seasonal weather, but because the hospital\n",
    "used the model to change patient health, the training data used to\n",
    "build the model is no longer consistent with the forward-looking\n",
    "health of the patients.  Thus, there is little reason to think that\n",
    "the model will continue to work as well going forward.\n",
    "\n",
    "Returning to our example,  let's suppose that the first eight elements from\n",
    "$\\mathcal{X}$ are twice as likely as the last eight. The following code is a\n",
    "function that generates elements from $\\mathcal{X}$ according to this\n",
    "distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "def get_sample(n=1):\n",
    "   if n==1:\n",
    "      return '{0:04b}'.format(np.random.choice(list(range(8))*2+list(range(8,16))))\n",
    "   else:\n",
    "      return [get_sample(1) for _ in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The function that returns random samples uses the\n",
    "`np.random.choice` function from Numpy which takes samples (with replacement)\n",
    "from the given iterable.  Because we want the first eight numbers to be twice\n",
    "as frequent as the rest, we simply repeat them in the iterable using\n",
    "`range(8)*2`. Recall that multiplying a Python list by an integer duplicates\n",
    "the entire list by that integer. It does not do element-wise multiplication as\n",
    "with Numpy arrays. If we wanted the first eight to be 10 times more frequent,\n",
    "then we would use `range(8)*10`, for example. This is a simple but powerful\n",
    "technique that requires very little code. Note that the `p` keyword argument in\n",
    "`np.random.choice` also provides an explicit way to specify more  complicated\n",
    "distributions.\n",
    "\n",
    "\n",
    "\n",
    " The next block applies the function definition $f$ to the\n",
    "sampled data to generate the training set consisting of eight elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "train=df.f.ix[get_sample(8)] # 8-element training set\n",
    "train.index.unique().shape    # how many unique elements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that even though there are eight elements, there is redundancy\n",
    "because these are drawn according to an underlying probability.  Otherwise, if\n",
    "we just got all sixteen different elements, we would have a training set\n",
    "consisting of the complete specification of $f$ and then we would therefore\n",
    "know what $h\\in \\mathcal{H}$ to pick! However, this effect gives us a clue as\n",
    "to how this will ultimately work. Given the elements in the training set,\n",
    "consider the set of elements from the hypothesis set that exactly match.  How\n",
    "to choose among these? The answer is it does not matter! Why? Because under the\n",
    "assumption that the prediction will be used in an environment that is\n",
    "determined by the same probability, getting something outside of the training\n",
    "set is just as likely as getting something inside the training set.  The size\n",
    "of the training set is key here --- the bigger the training set, the less\n",
    "likely that there will be real-world data that fall outside of it and the\n",
    "better $\\hat{f}$ will perform [^complexity].  The following code shows the\n",
    "elements of the training set in the context of all possible data.\n",
    "\n",
    "[^complexity]: This assumes that the hypothesis set is big enough to capture\n",
    "the entire training set (which it is for this example). We will discuss this\n",
    "trade-off in greater generality shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "df['fhat']=df.f.ix[train.index.unique()]\n",
    "df.fhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are `NaN` symbols where the training set had\n",
    "no values. For definiteness, we fill these in with zeros, although we\n",
    "can fill them with anything we want so long as whatever we do is not\n",
    "determined by the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "df.fhat.fillna(0,inplace=True) #final specification of fhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's pretend we have deployed this and generate some\n",
    "test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "test= df.f.ix[get_sample(50)]\n",
    "(df.ix[test.index]['fhat'] != test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows the error rate, given the probability\n",
    "mechanism that is generating the data.  The following Pandas-fu\n",
    "compares the overlap between the training set and the test set in the\n",
    "context of all possible data.  The `NaN` values show the rows where\n",
    "the test data had items absent in the training data. Recall that the\n",
    "method returns zero for these items.  As shown, sometimes this works\n",
    "in its favor, and sometimes not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([test.groupby(level=0).mean(), \n",
    "           train.groupby(level=0).mean()],\n",
    "          axis=1,\n",
    "          keys=['test','train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that where the test data and training data share\n",
    "elements, they agree. When the test set produced an unseen element, it\n",
    "produces a match or not.\n",
    "\n",
    "**Programming Tip.**\n",
    "\n",
    "The `pd.concat` function concatenates the two `Series` objects in the\n",
    "list. The `axis=1` means join the two objects along the columns where\n",
    "each newly created column is named according to the given `keys`. The\n",
    "`level=0` in the `groupby` for each of the `Series` objects  means\n",
    "group along the index. Because the index corresponds to the 4-bit\n",
    "elements, this accounts for repetition in the elements. The `mean`\n",
    "aggregation function computes the values of the function for each\n",
    "4-bit element. Because all functions in each  respective group have\n",
    "the same value, the `mean` just picks out that value\n",
    "because the average of a list of constants is that constant.\n",
    "\n",
    "\n",
    "\n",
    "Now, we are in position to ask how big the training set should be to achieve a\n",
    "level of performance. For example, on average, how many in-samples do we need\n",
    "for a given error rate? For this problem, we can ask how large (on average)\n",
    "must the training set be in order to capture *all* of the possibilities and\n",
    "achieve perfect out-of-sample error rates?  For this problem, this turns out\n",
    "to be sixty-three [^coupon].  Let's start over and retrain with these many\n",
    "in-samples.\n",
    "\n",
    "[^coupon]: This is a slight generalization of the classic coupon collector\n",
    "problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "train=df.f.ix[get_sample(63)] \n",
    "del df['fhat']   \n",
    "df['fhat']=df.f.ix[train.index.unique()]\n",
    "df.fhat.fillna(0,inplace=True) #final specification of fhat\n",
    "test= df.f.ix[get_sample(50)] \n",
    "(df.fhat.ix[test] != df.f.ix[test]).mean() # error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this bigger training set has a better error rate because\n",
    "it is able to identify the best element from the hypothesis set because the\n",
    "training set captured more of the complexity of the unknown $f$. This example\n",
    "shows the trade-offs between the size of the training set, the complexity of\n",
    "the target function, the probability structure of the data, and the size of the\n",
    "hypothesis set.\n",
    "\n",
    "## Theory of Generalization\n",
    "\n",
    "What we really want to know is how the our method will perform once deployed.\n",
    "It would be nice to have some kind of performance guarantee. In other words, we\n",
    "worked hard to minimize the errors in the training set, but what errors can we\n",
    "expect at deployment? In training, we minimized the\n",
    "in-sample error, $E_{\\texttt{in}}(\\hat{f}) $, but that's not good enough.  We\n",
    "want guarantees about the out-of-sample error, $ E_{\\texttt{out}}(\\hat{f})$.\n",
    "This is what *generalization* means in machine learning. The\n",
    "mathematical statement of this is the following,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left( \\lvert E_{\\texttt{out}}(\\hat{f})-E_{\\texttt{in}}(\\hat{f})\n",
    "\\rvert > \\epsilon \\right) < \\delta\n",
    "$$\n",
    "\n",
    " for a given $\\epsilon$ and $\\delta$. Informally, this says\n",
    "that the probability of the respective errors differing by more than a\n",
    "given $\\epsilon$ is less than some quantity, $\\delta$. This basically\n",
    "means that whatever the performance on the training set, it should\n",
    "probably  be pretty close to the corresponding performance once\n",
    "deployed.  Note that this does not say that the in-sample errors\n",
    "($E_{\\texttt{in}}$) are any good in an absolute sense.  It just says\n",
    "that we would not expect much different after deployment.  Thus,\n",
    "*good* generalization means no surprises after deployment, not\n",
    "necessarily good performance, by any means.  There are two main ways\n",
    "to get at this: cross-validation and probability inequalities.  Let's\n",
    "consider the latter first.  There are two entangled issues: the\n",
    "complexity of the hypothesis set and the probability of the data. It\n",
    "turns out we can separate these two by deriving a separate notion of\n",
    "complexity free from any particular data probability.\n",
    "\n",
    "\n",
    "**VC Dimension.** We first need a way to quantify model complexity.\n",
    "Following Wasserman [[wasserman2004all]](#wasserman2004all), let $\\mathcal{A}$\n",
    "be a\n",
    "class of sets and $F = \\left\\{x_1,x_2,\\ldots,x_n\\right\\}$, a set of\n",
    "$n$ data points. Then, we define\n",
    "\n",
    "$$\n",
    "N_{\\mathcal{A}}(F)=\\# \\left\\{ F \\cap A : A \\in \\mathcal{A}\\right\\}\n",
    "$$\n",
    "\n",
    " This counts the number of subsets of $F$ that can be extracted by the\n",
    "sets of $\\mathcal{A}$. The number of items in the set (i.e., cardinality) is\n",
    "noted by the $\\#$ symbol.  For example, suppose $F=\\left\\{1\\right\\}$ and\n",
    "$\\mathcal{A}=\\left\\{(x\\leq a)\\right\\}$.  In other words, $\\mathcal{A}$ consists\n",
    "of all intervals closed on the right and parameterized by $a$. In this case we\n",
    "have $N_{\\mathcal{A}}(F)=1$ because all elements can be extracted from $F$\n",
    "using $\\mathcal{A}$.\n",
    "\n",
    "\n",
    "The *shatter coefficient* is defined as,\n",
    "\n",
    "$$\n",
    "s(\\mathcal{A},n) = \\max_{F\\in \\mathcal{F}_n} N_{\\mathcal{A}}(F)\n",
    "$$\n",
    "\n",
    " where $\\mathcal{F}$ consists of all finite sets of size $n$. Note\n",
    "that this sweeps over all finite sets so we don't need to worry about any\n",
    "particular data set of finitely many points. The definition is concerned with\n",
    "$\\mathcal{A}$ and how its sets can pick off elements from the data set. A\n",
    "set $F$ is *shattered* by $\\mathcal{A}$ if it can pick out every element in it.\n",
    "This provides a sense of how the complexity in $\\mathcal{A}$ consumes data. In\n",
    "our last example, the set of half-closed intervals shattered every singleton\n",
    "set $\\left\\{x_1\\right\\}$.\n",
    "\n",
    "Now, we come to the main definition of the Vapnik-Chervonenkis\n",
    "[[vapnik2000nature]](#vapnik2000nature) dimension $d_{\\texttt{VC}}$ which\n",
    "defined as the largest\n",
    "$k$ for which $s(\\mathcal{A},n) = 2^k$, except in the case where\n",
    "$s(\\mathcal{A},n) = 2^n$ for which it is defined as  infinity. For our example\n",
    "where $F= \\left\\{x_1\\right\\}$, we already saw that $\\mathcal{A}$ shatters $F$.\n",
    "How about when $F=\\left\\{x_1,x_2\\right\\}$? Now, we have two points and we have\n",
    "to consider whether all subsets can be  extracted by $\\mathcal{A}$. In this\n",
    "case, there are four subsets,\n",
    "$\\left\\{\\o,\\left\\{x_1\\right\\},\\left\\{x_2\\right\\},\\left\\{x_1,x_2\\right\\}\n",
    "\\right\\}$.  Note that $\\o$ denotes the empty set. The empty set is easily\n",
    "extracted --- pick $a$ so that it is smaller than both\n",
    "$x_1$ and $x_2$.  Assuming that $x_1<x_2$, we can get the next set by\n",
    "choosing $x_1<a<x_2$. The last set is likewise do-able by choosing $x_2<a$.\n",
    "The problem is that we cannot capture the third set, $\\left\\{x_2\\right\\}$,\n",
    "without capturing $x_1$ as well.  This means that we cannot shatter any\n",
    "finite set with $n=2$ using $\\mathcal{A}$. Thus, $d_{\\texttt{VC}}=1$.\n",
    "\n",
    "Here is the climatic result\n",
    "\n",
    "$$\n",
    "E_{\\texttt{out}}(\\hat{f}) \\leq E_{\\texttt{in}}(\\hat{f}) + \\sqrt{\\frac{8}{n}\\ln\n",
    "\\left( \\frac{4 ((2 n)^{ d_{\\texttt{VC}} }+1)}{\\delta} \\right) }\n",
    "$$\n",
    "\n",
    " with probability at least $1-\\delta$. This basically says that the\n",
    "expected out-of-sample error can be no worse than the in-sample error plus a\n",
    "penalty due to the complexity of the hypothesis set. The expected in-sample\n",
    "error\n",
    "comes from the training set but the complexity penalty comes\n",
    "from just the hypothesis set, so we have disentangled these two issues.\n",
    "\n",
    "A general result like this, for which we do not worry about the probability of\n",
    "the data, is certain to be pretty generous, but nonetheless, it tells us how\n",
    "the complexity penalty enters into the out-of-sample error.  In other words,\n",
    "the bound on $E_{\\texttt{out}}(\\hat{f})$ gets worse for a more complex\n",
    "hypothesis set. Thus, this generalization bound is a useful guideline but not\n",
    "very practical if we want to get a good estimate of\n",
    "$E_{\\texttt{out}}(\\hat{f})$.\n",
    "\n",
    "## Worked Example for Generalization/Approximation Complexity\n",
    "\n",
    "The stylized curves in  [Figure](#fig:learning_theory_tmp_004) illustrate the\n",
    "idea that there is some optimal point of complexity that represents the best\n",
    "generalization given the training set.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-machine_learning/learning_theory_tmp_004.png, width=500\n",
    "frac=0.80] In the ideal situation, there is a *best model* that represents the\n",
    "optimal trade-off between complexity and error. This is shown by the vertical\n",
    "line.   <div id=\"fig:learning_theory_tmp_004\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:learning_theory_tmp_004\"></div>\n",
    "\n",
    "<p>In the ideal situation, there is a <em>best model</em> that represents the\n",
    "optimal trade-off between complexity and error. This is shown by the vertical\n",
    "line.</p>\n",
    "<img src=\"fig-machine_learning/learning_theory_tmp_004.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "To get a firm handle on these curves, let's develop a simple one-dimensional\n",
    "machine learning method and go through the steps to create this graph.  Let's\n",
    "suppose we have a training set consisting of x-y pairs\n",
    "$\\left\\{(x_i,y_i)\\right\\}$. Our method groups the x-data into intervals\n",
    "and then averages the y-data in those intervals. Predicting for new x-data\n",
    "means simply identifying the interval containing the new data then reporting\n",
    "the corresponding value. In other words, we are building a simple\n",
    "one-dimensional, nearest neighbor classifier. For example, suppose the training\n",
    "set\n",
    "x-data is the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "train=DataFrame(columns=['x','y'])\n",
    "train['x']=np.sort(np.random.choice(range(2**10),size=90))\n",
    "train.x.head(10) # first ten elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we took a random set of 10-bit integers. To group\n",
    "these\n",
    "into, say, ten intervals, we simply use Numpy reshape as in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "train.x.reshape(10,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where every row is one of the groups. Note that the range of each\n",
    "group (i.e., length of the interval) is not preassigned, and is learned from\n",
    "the training data. For this example, the y-values correspond to the number of\n",
    "ones in the bit representation of the x-values. The following code defines this\n",
    "target function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "f_target=np.vectorize(lambda i:sum(map(int,i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The above function uses `np.vectorize` which is a convenience method in Numpy\n",
    "that converts plain Python functions into Numpy versions. This basically saves\n",
    "additional looping semantics and makes it easier to use with other Numpy\n",
    "arrays and functions.\n",
    "\n",
    "\n",
    "\n",
    " Next, we create the bit representations of all of the x-data below and\n",
    "then complete training set y-values,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [],
   "source": [
    "train['xb']= train.x.map('{0:010b}'.format)\n",
    "train.y=train.xb.map(f_target)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train on this data, we just group by the specified amount and then\n",
    "average the y-data over each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [],
   "source": [
    "train.y.reshape(10,-1).mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `axis=1` keyword argument just means average across the\n",
    " columns. So far, this defines the training.  To predict using this\n",
    "method, we have to extract the edges from each of the groups and then fill in\n",
    "with the group-wise mean we just computed for `y`. The following code extracts\n",
    "the edges of each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [],
   "source": [
    "le,re=train.x.reshape(10,-1)[:,[0,-1]].T\n",
    "print (le) # left edge of group\n",
    "print (re) # right edge of group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the group-wise means and assign them to their\n",
    "respective edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    }
   },
   "outputs": [],
   "source": [
    "val = train.y.reshape(10,-1).mean(axis=1).round()\n",
    "func = pd.Series(index=range(1024))\n",
    "func[le]=val    # assign value to left edge\n",
    "func[re]=val    # assign value to right edge\n",
    "func.iloc[0]=0  # default 0 if no data\n",
    "func.iloc[-1]=0 # default 0 if no data\n",
    "func.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Pandas `Series` object automatically fills in\n",
    "unassigned values with `NaN`. We have thus far only filled in values at the\n",
    "edges of the groups. Now, we need to fill in the intermediate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [],
   "source": [
    "fi=func.interpolate('nearest')\n",
    "fi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `interpolate` method of the `Series` object can apply a wide\n",
    "variety of powerful interpolation methods, but we only need the simple nearest\n",
    "neighbor method to create our piecewise approximant.\n",
    "[Figure](#fig:learning_theory_001) shows how this looks for the training data we\n",
    "have\n",
    "created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import subplots, mean, arange, setp\n",
    "fig,ax=subplots()\n",
    "fig.set_size_inches((10,5))\n",
    "_=ax.axis(xmax=1024,ymax=10)\n",
    "v=ax.stem(train.x,train.y,markerfmt='go',linefmt='g-')\n",
    "_=setp(v,color='gray')\n",
    "_=fi.plot(ax=ax,lw=4.,color='k')\n",
    "_=ax.set_xlabel('$X$',fontsize=20)\n",
    "_=ax.set_ylabel('$y$',fontsize=22)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/learning_theory_001.png, width=500\n",
    "frac=0.85] The vertical lines show the training data and the thick black line is\n",
    "the approximant we have learned from the training data. <div\n",
    "id=\"fig:learning_theory_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:learning_theory_001\"></div>\n",
    "\n",
    "<p>The vertical lines show the training data and the thick black line is the\n",
    "approximant we have learned from the training data.</p>\n",
    "<img src=\"fig-machine_learning/learning_theory_001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "Now, with all that established, we can now  draw the curves for this machine\n",
    "learning method. Instead of partitioning the training data for cross-validation\n",
    "(which we'll discuss later), we can simulate test data using the same mechanism\n",
    "as for the training data, as shown next,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    }
   },
   "outputs": [],
   "source": [
    "test=pd.DataFrame(columns=['x','xb','y'])\n",
    "test['x']=np.random.choice(range(2**10),size=500)\n",
    "test.xb= test.x.map('{0:010b}'.format)\n",
    "test.y=test.xb.map(f_target)\n",
    "test.sort_values('x',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curves are the respective errors for the training data\n",
    "and the testing data. For our error measure, we use the mean-squared-error,\n",
    "\n",
    "$$\n",
    "E_{\\texttt{out}} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{f}(x_i) - y_i)^2\n",
    "$$\n",
    "\n",
    " where $\\left\\{(x_i,y_i)\\right\\}_{i=1}^n$ come from the test data. The\n",
    "in-sample error ($E_{\\texttt{in}}$) is defined the same except for the\n",
    "in-sample data.  In this example, the size of each group is proportional to\n",
    "$d_{\\texttt{VC}}$, so the more groups we choose, the more complexity in the\n",
    "fitting.  Now, we have all the ingredients to understand the trade-offs of\n",
    "complexity versus error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "21"
    }
   },
   "outputs": [],
   "source": [
    "n=train.shape[0]\n",
    "divisors=arange(1,n+1)[(n % arange(1,n+1))==0]\n",
    "def atrainer(train,dvc):\n",
    "    le,re=train.x.reshape(dvc,-1)[:,[0,-1]].T\n",
    "    val = train.y.reshape(dvc,-1).mean(axis=1).round()\n",
    "    func = pd.Series(index=range(1024))\n",
    "    func[le]=val\n",
    "    func[re]=val\n",
    "    func.iloc[0]=0\n",
    "    func.iloc[-1]=0\n",
    "    fi=func.interpolate('nearest')\n",
    "    return fi\n",
    "\n",
    "otrn=[]; otst=[]\n",
    "for i in divisors: # loop over divisors\n",
    "    fi=atrainer(train,i)\n",
    "    otrn.append((mean((fi[train.x].values-train.y.values)**2)))\n",
    "    otst.append((mean((fi[test.x].values-test.y.values)**2)))\n",
    "\n",
    "fig,ax=subplots()\n",
    "fig.set_size_inches((10,6))\n",
    "_=ax.plot(divisors,otrn,'--s',label='train',color='k')\n",
    "_=ax.plot(divisors,otst,'-o',label='test',color='gray')\n",
    "_=ax.fill_between(divisors,otrn,otst,color='gray',alpha=.3)\n",
    "_=ax.legend(loc=0)\n",
    "_=ax.set_xlabel('Complexity',fontsize=22)\n",
    "_=ax.set_ylabel('Mean-squared-error',fontsize=22)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Figure](#fig:learning_theory_002) shows the curves for our\n",
    "one-dimensional clustering method.  The dotted line shows the\n",
    "mean-squared-error on the training set and the other line shows the\n",
    "same for the test data. The shaded region is the *complexity penalty*\n",
    "of this method.  Note that with enough complexity, the method can\n",
    "exactly memorize the testing data, but that only penalizes the testing\n",
    "error ($E_{\\texttt{out}}$). This effect is exactly what the\n",
    "Vapnik-Chervonenkis theory expresses. The horizontal axis is\n",
    "proportional to the VC-dimension. In this case, complexity boils down\n",
    "to the number of intervals used in the sectioning. At the far right,\n",
    "we have as many intervals as there are elements in the data set,\n",
    "meaning that every element is wrapped in its own interval. The average\n",
    "value of the data in that interval is therefore just the corresponding\n",
    "$y$ value because there are no other elements to average over. The\n",
    "Jupyter/IPython notebook corresponding to the section has the code to\n",
    "generate these curves so you can see how these curves change with\n",
    "bigger or smaller data sets.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-machine_learning/learning_theory_002.png, width=500\n",
    "frac=0.85] The dotted line shows the mean-squared-error on the training set and\n",
    "the other line shows the same for the test data. The shaded region is the\n",
    "*complexity penalty* of this method. Note that as the complexity of the model\n",
    "increases, the training error decreases, and the method essentially memorizes\n",
    "the data. However, this improvement in training error comes at the cost of\n",
    "larger testing error. <div id=\"fig:learning_theory_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:learning_theory_002\"></div>\n",
    "\n",
    "<p>The dotted line shows the mean-squared-error on the training set and the\n",
    "other line shows the same for the test data. The shaded region is the\n",
    "<em>complexity penalty</em> of this method. Note that as the complexity of the\n",
    "model increases, the training error decreases, and the method essentially\n",
    "memorizes the data. However, this improvement in training error comes at the\n",
    "cost of larger testing error.</p>\n",
    "<img src=\"fig-machine_learning/learning_theory_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "Before we leave this problem, there is another way to visualize the\n",
    "performance of our learning method. This problem can be thought of as\n",
    "a multi-class identification problem. Given a 10-bit integer, the\n",
    "number of ones in its binary representation is in one of the classes\n",
    "$\\left\\{0,1,\\ldots,10\\right\\}$.  The output of the model tries to put\n",
    "each integer in its respective class. How well this was done can be\n",
    "visualized using a *confusion matrix* as shown in the next code block,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "22"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cmx=confusion_matrix(test.y.values,fi[test.x].values)\n",
    "print(cmx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows of this $10 \\times 10$ matrix show what the true class was\n",
    "and the columns indicate the class that the model predicted.  The numbers in\n",
    "the matrix indicate the number of times that association was made. For example,\n",
    "the first row shows that there was one entry in the test set with no ones in\n",
    "its binary representation (i.e, namely the number zero) and it was correctly\n",
    "classified (namely, it is in the first row, first column of the matrix). The\n",
    "second row shows there were four entries total in the test set with a binary\n",
    "representation containing exactly a single one. This was incorrectly classified\n",
    "as the 0-class (i.e, first column) once, the 2-class (third column) once, the\n",
    "4-class (fifth column) once, and the 5-class (sixth column) once.  It was never\n",
    "classified correctly because the second column is zero for this row. In other\n",
    "words, the diagonal entries show the number of times it was correctly\n",
    "classified.\n",
    "\n",
    "Using this matrix, we can easily estimate the true-detection\n",
    "probability that we covered earlier in our hypothesis testing section,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "23"
    }
   },
   "outputs": [],
   "source": [
    "print(cmx.diagonal()/cmx.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the first element is the probability of detecting `0`\n",
    "when `0` is in force, the second element is the probability of detecting `1`\n",
    "when `1` is in force, and so on. We can likewise compute the false-alarm rate\n",
    "for each of the classes in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "24"
    }
   },
   "outputs": [],
   "source": [
    "print (cmx.sum(axis=0) - cmx.diagonal())/(cmx.sum()-cmx.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The Numpy `sum` function can sum across a particular axis or, if the\n",
    "axis is unspecified, will sum all entries of the array.\n",
    "\n",
    "\n",
    "\n",
    " In this case, the first element is the probability that `0`\n",
    "is declared when another category is in force, the next element is the\n",
    "probability that `1` is declared when another category is in force,\n",
    "and so on.. For a decent classifier, we want a true-detection\n",
    "probability to be greater than the corresponding false-alarm rate,\n",
    "otherwise the classifier is no better than a coin-flip. Note that, at\n",
    "the time of this writing, Scikit-learn  has limited tools for this\n",
    "kind of multiple class classification task.\n",
    "\n",
    "The missing feature of this problem, from the learning\n",
    "algorithm standpoint, is that we did not supply the bit representation\n",
    "of every element which was used to derive the target variable, $y$.\n",
    "Instead, we just used the integer value of each of the 10-bit numbers,\n",
    "which essentially concealed the mechanism for creating the $y$ values.\n",
    "In other words, there was a unknown transformation from the input\n",
    "space $\\mathcal{X}$ to $\\mathcal{Y}$ that the learning algorithm had\n",
    "to overcome, but that it could not overcome, at least not without\n",
    "memorizing the training data. This lack of knowledge is a key issue in\n",
    "all machine learning problems, although we have made it explicit here\n",
    "with this stylized example. This means that there may be one or more\n",
    "transformations from $\\mathcal{X} \\rightarrow \\mathcal{X}^\\prime$ that\n",
    "can help learning algorithm get traction on the so-transformed space\n",
    "while providing a better trade-off between generalization and\n",
    "approximation than could have been achieved otherwise. Finding such\n",
    "transformations is called *feature engineering*.\n",
    "\n",
    "\n",
    "## Cross-Validation\n",
    "<div id=\"ch:ml:sec:cv\"></div>\n",
    "\n",
    "In the last section, we explored a stylized machine learning example\n",
    "to understand the issues of complexity in machine learning. However,\n",
    "to get an estimate of out-of-sample errors, we simply generated more\n",
    "synthetic data. In practice, this is not an option, so we need to\n",
    "estimate these errors from the training set itself. This is what\n",
    "cross-validation does.  The simplest form of cross-validation is\n",
    "k-fold validation. For example, if $K=3$, then the training data is\n",
    "divided into three sections wherein each of the three sections is used\n",
    "for testing and the remaining two are used for training.  This is\n",
    "implemented in Scikit-learn as in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "25"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "data =np.array(['a',]*3+['b',]*3+['c',]*3) # example \n",
    "print (data)\n",
    "kf = KFold(3)\n",
    "for train_idx,test_idx in kf.split(data):\n",
    "   print (train_idx,test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we construct a sample data array and then see\n",
    "how `KFold` splits it up into indicies for training and testing, respectively.\n",
    "Notice that there are no duplicated elements in each row between training and\n",
    "testing indicies. To examine the elements of the data set in each category, we\n",
    "simply\n",
    "use each of the indicies as in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "26"
    }
   },
   "outputs": [],
   "source": [
    "for train_idx,test_idx in kf.split(data):\n",
    "   print('training', data[ train_idx ])\n",
    "   print('testing' , data[ test_idx ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows how each group is used in turn for training/testing. There\n",
    "is no random shuffling of the data unless the `shuffle` keyword argument is\n",
    "given.  The error over the test set is th *cross-validation error*. The idea is\n",
    "to postulate models of differing complexity and then pick the one with the best\n",
    "cross-validation error. For example, suppose we had the following sine wave\n",
    "data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "27"
    }
   },
   "outputs": [],
   "source": [
    "xi = np.linspace(0,1,30)\n",
    "yi = np.sin(2*np.pi*xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we want to fit this with polynomials of increasing order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "28"
    }
   },
   "outputs": [],
   "source": [
    "fig,axs=subplots(2,2,sharex=True,sharey=True)\n",
    "deg = 1 # polynomial degree\n",
    "for ax,(train_idx,test_idx) in zip(axs.flat,KFold(len(xi),4)):\n",
    "    _=ax.plot(xi,yi,xi[train_idx],yi[train_idx],'ok',color='k')\n",
    "    p = np.polyfit(xi[train_idx],yi[train_idx],deg)\n",
    "    pval = np.polyval(p,xi)\n",
    "    _=ax.plot(xi,pval,'--k')\n",
    "    error = np.mean((pval[test_idx]-yi[test_idx])**2)\n",
    "    _=ax.set_title('degree=%d;error=%3.3g'%(deg,error))\n",
    "    _=ax.fill_between(xi[test_idx],pval[test_idx],yi[test_idx],color='gray',alpha=.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/learning_theory_003.png, width=500\n",
    "frac=0.85] This shows the folds and errors for the linear model. The shaded\n",
    "areas show the errors in each respective test set (i.e, *cross-validation\n",
    "scores*) for the linear model. <div id=\"fig:learning_theory_003\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:learning_theory_003\"></div>\n",
    "\n",
    "<p>This shows the folds and errors for the linear model. The shaded areas show\n",
    "the errors in each respective test set (i.e, <em>cross-validation scores</em>)\n",
    "for the linear model.</p>\n",
    "<img src=\"fig-machine_learning/learning_theory_003.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "[Figure](#fig:learning_theory_003) shows the individual folds in each\n",
    "panel.  The circles represent the training data. The diagonal line is\n",
    "the fitted polynomial. The gray shaded areas indicate the regions of\n",
    "errors between the fitted polynomial and the held-out testing data.\n",
    "The larger the gray area, the bigger the cross-validation errors,\n",
    "as are reported in the title of each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "29"
    }
   },
   "outputs": [],
   "source": [
    "fig,axs=subplots(2,2,sharex=True,sharey=True)\n",
    "deg = 2 # polynomial degree\n",
    "for ax,(train_idx,test_idx) in zip(axs.flat,KFold(len(xi),4)):\n",
    "    _=ax.plot(xi,yi,xi[train_idx],yi[train_idx],'ok',color='k')\n",
    "    p = np.polyfit(xi[train_idx],yi[train_idx],deg)\n",
    "    pval = np.polyval(p,xi)\n",
    "    _=ax.plot(xi,pval,'--k')\n",
    "    error = np.mean((pval[test_idx]-yi[test_idx])**2)\n",
    "    _=ax.set_title('degree=%d;error=%3.3g'%(deg,error))\n",
    "    _=ax.fill_between(xi[test_idx],pval[test_idx],yi[test_idx],color='gray',alpha=.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/learning_theory_004.png, width=500\n",
    "frac=0.85] This shows the folds and errors as in\n",
    "[Figure](#fig:learning_theory_002) and\n",
    "[fig:learning_theory_003](#fig:learning_theory_003). The shaded areas show the\n",
    "errors in each respective test set for the quadratic model. <div\n",
    "id=\"fig:learning_theory_004\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:learning_theory_004\"></div>\n",
    "\n",
    "<p>This shows the folds and errors as in [Figure](#fig:learning_theory_002) and\n",
    "[fig:learning_theory_003](#fig:learning_theory_003). The shaded areas show the\n",
    "errors in each respective test set for the quadratic model.</p>\n",
    "<img src=\"fig-machine_learning/learning_theory_004.png\" width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "30"
    }
   },
   "outputs": [],
   "source": [
    "fig,axs=subplots(2,2,sharex=True,sharey=True)\n",
    "deg = 3 # polynomial degree\n",
    "for ax,(train_idx,test_idx) in zip(axs.flat,KFold(len(xi),4)):\n",
    "    _=ax.plot(xi,yi,xi[train_idx],yi[train_idx],'ok',color='k')\n",
    "    p = np.polyfit(xi[train_idx],yi[train_idx],deg)\n",
    "    pval = np.polyval(p,xi)\n",
    "    _=ax.plot(xi,pval,'--k')\n",
    "    error = np.mean((pval[test_idx]-yi[test_idx])**2)\n",
    "    _=ax.set_title('degree=%d;error=%3.3g'%(deg,error))\n",
    "    _=ax.fill_between(xi[test_idx],pval[test_idx],yi[test_idx],color='gray',alpha=.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig-machine_learning/learning_theory_005.png, width=500\n",
    "frac=0.85] This shows the folds and errors. The shaded areas show the errors in\n",
    "each respective test set for the cubic model. <div\n",
    "id=\"fig:learning_theory_005\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:learning_theory_005\"></div>\n",
    "\n",
    "<p>This shows the folds and errors. The shaded areas show the errors in each\n",
    "respective test set for the cubic model.</p>\n",
    "<img src=\"fig-machine_learning/learning_theory_005.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "After reviewing the last three figures and averaging the cross-validation\n",
    "errors, the one with the least average error is declared the winner. Thus,\n",
    "cross-validation provides a method of using a\n",
    "single data set to make claims about unseen out-of-sample data insofar as the\n",
    "model with the best complexity can be determined.  The entire process to\n",
    "generate the above figures can be captured using `cross_val_score` as shown for\n",
    "the linear regression (compare the output with the values in the titles in each\n",
    "panel of  [Figure](#fig:learning_theory_003)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "31"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import  make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "Xi = xi.reshape(-1,1) # refit column-wise\n",
    "Yi = yi.reshape(-1,1)\n",
    "lf = LinearRegression()\n",
    "scores = cross_val_score(lf,Xi,Yi,cv=4,\n",
    "                         scoring=make_scorer(mean_squared_error))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "The `make_scorer` function is a wrapper that enables `cross_val_score` to\n",
    "compute scores from the given estimator's output.\n",
    "\n",
    "\n",
    "\n",
    " The process can be further automated by using a pipeline as\n",
    "in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "32"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polyfitter = Pipeline([('poly', PolynomialFeatures(degree=3)),\n",
    "                  ('linear', LinearRegression())])\n",
    "polyfitter.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  `Pipeline` object is a way of stacking standard steps\n",
    "into one big estimator, while respecting the usual `fit` and `predict`\n",
    "interfaces. The output of the `get_params` function contains the\n",
    "polynomial degrees we previously looped over to create\n",
    "[Figure](#fig:learning_theory_003), etc. We will use these named parameters\n",
    "in the next code block.  To do this automatically using this\n",
    "`polyfitter` estimator, we need the Grid Search Cross Validation\n",
    "object, `GridSearchCV`.  The next step is to use this to create the\n",
    "grid of parameters we want to loop over as in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "33"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "gs=GridSearchCV(polyfitter,{'poly__degree':[1,2,3]},cv=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gs` object will loop over the polynomial degrees up to\n",
    "cubic using four-fold cross validation `cv=4`, like we did manually\n",
    "earlier. The `poly__degree` item comes from the previous `get_params`\n",
    "call.  Now, we just apply the usual `fit` method on the training data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "34"
    }
   },
   "outputs": [],
   "source": [
    "_=gs.fit(Xi,Yi)\n",
    "gs.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the scores shown correspond to the cross validation scores\n",
    "for each of the parameters (e.g., polynomial degrees) using four-fold\n",
    "cross-validation.  Note that the higher scores are better here and\n",
    "the cubic polynomial is best, as we observed earlier. The default\n",
    "$R^2$ metric is used for the scoring in this case as opposed to\n",
    "mean-squared-error. The validation results of this pipeline for the\n",
    "quadratic fit are shown in [Figure](#fig:learning_theory_004), and\n",
    "for the cubic fit, in [Figure](#fig:learning_theory_005). This can\n",
    "be changed by passing the\n",
    "`scoring=make_scorer(mean_squared_error)` keyword argument to\n",
    "`GridSearchCV`.  There is also `RandomizedSearchCV` that does\n",
    "does not necessarily evaluate every point on the grid and\n",
    "instead randomly samples the grid according to an input\n",
    "probability distribution. This is very useful for a large number\n",
    "of hyper-parameters.\n",
    "\n",
    "## Bias and Variance\n",
    "\n",
    "So far, we have been thinking about the average error in terms of in-samples\n",
    "and out-samples, but this depends on a particular training data set. What we\n",
    "want is a concept that extends to *all* possible training data and captures the\n",
    "performance of the estimator in that setting. For example, our ultimate\n",
    "estimator, $\\hat{f}$ is derived from a particular set of training data\n",
    "($\\mathcal{D}$) and is thus denoted, $\\hat{f}_{\\mathcal{D}}$. This makes the\n",
    "out-of-sample error explicitly, $E_{\\texttt{out}}(\\hat{f}_{\\mathcal{D}})$.  To\n",
    "eliminate the dependence on a particular set of training data set, we have to\n",
    "compute the expectation across all training data sets,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\mathcal{D}}E_{\\texttt{out}}(\\hat{f}_{\\mathcal{D}})=\n",
    "\\texttt{bias}+\\texttt{var}\n",
    "$$\n",
    "\n",
    " where\n",
    "\n",
    "$$\n",
    "\\texttt{bias}(x)= (\\overline{\\hat{f}}(x)-f(x))^2\n",
    "$$\n",
    "\n",
    " and\n",
    "\n",
    "$$\n",
    "\\texttt{var}(x)=\n",
    "\\mathbb{E}_{\\mathcal{D}}(\\hat{f}_{\\mathcal{D}}(x)-\\overline{\\hat{f}}(x))^2\n",
    "$$\n",
    "\n",
    " and where $\\overline{\\hat{f}}$ is the mean of all estimators for all\n",
    "data sets. There is nothing to say that such a mean is an estimator that could\n",
    "have arisen from any *particular* training data, however. It just implies that\n",
    "for any particular point $x$, the mean of the values of all the estimators is\n",
    "$\\overline{\\hat{f}}(x)$.  Therefore, $\\texttt{bias}$ captures the sense\n",
    "that, even if all possible data were presented to the learning method, it\n",
    "would still differ from the target function by this amount.  On the other\n",
    "hand $\\texttt{var}$ shows the variation in the final hypothesis, depending\n",
    "on the training data set, notwithstanding the target function.  Thus, the\n",
    "tension between approximation and generalization is captured by these two\n",
    "terms.  For example, suppose there is only one hypothesis. Then,\n",
    "$\\texttt{var}=0$ because there can be no variation due to a particular set\n",
    "of training data because no matter what that training data is, the learning\n",
    "method always selects the one and only hypothesis. In this case, the bias\n",
    "could be very large, because there is no opportunity for the learning method\n",
    "to alter the hypothesis due to the training data, and the method can only\n",
    "ever pick the single hypothesis!\n",
    "\n",
    "Let's construct an example to make this concrete. Suppose we have a hypothesis\n",
    "set consisting of all linear regressions without an intercept term,  $h(x)=a\n",
    "x$. The training data consists of only two points $\\left\\{(x_i,\\sin(\\pi\n",
    "x_i))\\right\\}_{i=1}^2$ where $x_i$ is drawn uniformly from the interval\n",
    "$[-1,1]$. From the section [ch:stats:sec:reg](#ch:stats:sec:reg) on linear\n",
    "regression, we know that\n",
    "the solution for $a$ is the following,\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:sola\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "a = \\frac{\\mathbf{x}^T \\mathbf{y}}{\\mathbf{x}^T \\mathbf{x}}\n",
    "\\label{eq:sola} \\tag{1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    " where $\\mathbf{x}=[x_1,x_2]$ and $\\mathbf{y}=[y_1,y_2]$. The\n",
    "$\\overline{\\hat{f}}(x)$ represents the solution over all possible sets\n",
    "of training data for a fixed $x$. The following code shows how to\n",
    "construct the training data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "35"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def gen_sindata(n=2):\n",
    "    x=stats.uniform(-1,1) # define random variable\n",
    "    v = x.rvs((n,1)) # generate sample\n",
    "    y = np.sin(np.pi*v) # use sample for sine\n",
    "    return (v,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, using Scikit-learn's `LinearRegression` object, we\n",
    "can compute the $a$ parameter. Note that we have to set\n",
    "`fit_intercept=False` keyword to suppress the default automatic\n",
    "fitting of the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "36"
    }
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(fit_intercept=False)\n",
    "lr.fit(*gen_sindata(2))\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- *foo* -->\n",
    "\n",
    "**Programming Tip.**\n",
    "\n",
    "Note that we designed `gen_sindata` to return a tuple to use the automatic\n",
    "unpacking feature of Python functions in `lr.fit(*gen_sindata())`.  In other\n",
    "words, using the asterisk notation means we don't have to separately assign the\n",
    "outputs of `gen_sindata` before using them for `lr.fit`.\n",
    "\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig-machine_learning/learning_theory_006.png, width=500\n",
    "frac=0.75] For a two-element training set consisting of the points shown, the\n",
    "line is the best fit over the hypothesis set, $h(x)=a x$. <div\n",
    "id=\"fig:learning_theory_006\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:learning_theory_006\"></div>\n",
    "\n",
    "<p>For a two-element training set consisting of the points shown, the line is\n",
    "the best fit over the hypothesis set, $h(x)=a x$.</p>\n",
    "<img src=\"fig-machine_learning/learning_theory_006.png\" width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "37"
    }
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(fit_intercept=False)\n",
    "xi=np.linspace(-1,1,50)\n",
    "yi= np.sin(np.pi*xi)\n",
    "xg,yg = gen_sindata()\n",
    "fig,ax=subplots()\n",
    "_=lr.fit(xg,yg)\n",
    "_=ax.plot(xi,yi,'--k',label='target')\n",
    "_=ax.plot(xg,yg,'o',ms=10,color='gray')\n",
    "_=ax.plot(xi,lr.predict(xi.reshape(-1,1)),color='k',label='best fit')\n",
    "_=ax.set_title('$a=%3.3g$'%(lr.coef_),fontsize=24)\n",
    "_=ax.set_xlabel(r'$x$',fontsize=28)\n",
    "_=ax.set_ylabel(r'$y$',fontsize=28)\n",
    "_=ax.legend(fontsize=18,loc=0)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, $\\overline{\\hat{f}}(x) = \\overline{a}x$, where\n",
    "$\\overline{a}$ the the expected value of the parameter over *all* possible\n",
    "training data sets.  Using our knowledge of probability, we can write this out\n",
    "explicitly as the following,\n",
    "\n",
    "$$\n",
    "\\overline{a} = \\mathbb{E}\\left(\\frac{x_1\\sin(\\pi x_1)+ x_2\\sin(\\pi x_2)\n",
    "}{x_1^2+x_2^2}\\right)\n",
    "$$\n",
    "\n",
    " where $\\mathbf{x}=[x_1,x_2]$ and $\\mathbf{y}=[\\sin(\\pi x_1),\\sin(\\pi\n",
    "x_2)]$ in Equation [eq:sola](#eq:sola). However, computing this expectation\n",
    "analytically is hard, but for this specific situation, $\\overline{a} \\approx\n",
    "1.43$. To get this value using simulation, we just loop over the process,\n",
    "collect the outputs, and the average them as in the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "38"
    }
   },
   "outputs": [],
   "source": [
    "a_out=[] # output container\n",
    "for i in range(100):\n",
    "    _=lr.fit(*gen_sindata(2)) \n",
    "    a_out.append(lr.coef_[0,0])\n",
    "\n",
    "np.mean(a_out) # approx 1.43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- * -->\n",
    "\n",
    " Note that you may have to loop over many more iterations to get\n",
    "close to the purported value. The $\\texttt{var}$ requires the variance of $a$,\n",
    "\n",
    "$$\n",
    "\\texttt{var}(x) = \\mathbb{E}((a-\\overline{a})x)^2 = x^2\n",
    "\\mathbb{E}(a-\\overline{a})^2 \\approx 0.71 x^2\n",
    "$$\n",
    "\n",
    " The $\\texttt{bias}$ is the following,\n",
    "\n",
    "$$\n",
    "\\texttt{bias}(x) = (\\sin(\\pi x)-\\overline{a}x)^2\n",
    "$$\n",
    "\n",
    " [Figure](#fig:learning_theory_007) shows the $\\texttt{bias}$,\n",
    "$\\texttt{var}$, and mean-squared-error for this problem. Notice that there is\n",
    "zero bias and zero variance when $x=0$. This is because the learning method\n",
    "cannot help but get that correct because all the hypotheses happen to match the\n",
    "value of the target function at that point! Likewise, the $\\texttt{var}$ is\n",
    "zero because all possible pairs, which constitute the training data, are fitted\n",
    "through zero because $h(x)=a x$ has no choice but to go through zero. The\n",
    "errors are worse at the end points. As we discussed in our statistics chapter,\n",
    "those points have the most leverage against the hypothesized models and result\n",
    "in the worst errors. Notice that reducing the edge-errors depends on getting\n",
    "exactly those points near the edges as training data. The sensitivity to a\n",
    "particular data set is reflected in this behavior.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-machine_learning/learning_theory_007.png, width=500\n",
    "frac=0.8] These curves decompose the mean squared error into its constituent\n",
    "bias and variance for this example.  <div id=\"fig:learning_theory_007\"></div>\n",
    "-->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:learning_theory_007\"></div>\n",
    "\n",
    "<p>These curves decompose the mean squared error into its constituent bias and\n",
    "variance for this example.</p>\n",
    "<img src=\"fig-machine_learning/learning_theory_007.png\" width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "39"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "fig.set_size_inches((8,5))\n",
    "_=ax.plot(xi,(1.43*xi-yi)**2,'--k',label='bias(x)')\n",
    "_=ax.plot(xi,0.71*(xi)**2,':k',label='var(x)',lw=3.)\n",
    "_=ax.plot(xi,(1.43*xi-yi)**2+0.71*(xi)**2,color='k',lw=4,label='MSE')\n",
    "_=ax.legend(fontsize=18,loc=0)\n",
    "_=ax.set_ylabel('Mean Squared Error (MSE)',fontsize=16)\n",
    "_=ax.set_xlabel('x',fontsize=18)\n",
    "_=ax.tick_params(labelsize='x-large')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we had more than two points in the training data? What would happen to\n",
    "$\\texttt{var}$ and $\\texttt{bias}$?  Certainly, the $\\texttt{var}$ would\n",
    "decrease because it would be harder and harder to generate training data sets\n",
    "that would be substantially different from each other. The bias would also\n",
    "decrease because more points in the training data means better approximation of\n",
    "the sine function over the interval. What would happen if we changed the\n",
    "hypothesis set to include more complex polynomials?  As we have already seen\n",
    "with our polynomial regression earlier in this chapter, we would see the same\n",
    "overall effect as here, but with relatively smaller absolute errors and the\n",
    "same edge effects we noted earlier. The corresponding Jupyter/IPython Notebook\n",
    "has the source code to help you can try these ideas and see for yourself.\n",
    "\n",
    "## Learning Noise\n",
    "\n",
    "We have thus far not considered the effect of noise in our analysis\n",
    "of learning. The following example should help resolve this. Let's suppose\n",
    "we have the following scalar target function,\n",
    "\n",
    "$$\n",
    "y(\\mathbf{x})=\\mathbf{w}_o^T \\mathbf{x} + \\eta\n",
    "$$\n",
    "\n",
    " where $\\eta \\sim \\mathcal{N}(0,\\sigma^2)$ is an additive noise term\n",
    "and $\\mathbf{w}, \\mathbf{x} \\in \\mathbb{R}^d$. Furthermore, we have $n$\n",
    "measurements of $y$. This means the training set consists of $\\lbrace\n",
    "(\\mathbf{x}_i,y_i) \\rbrace_{i=1}^n$. Stacking the measurements together into a\n",
    "vector format,\n",
    "\n",
    "$$\n",
    "\\mathbf{y}=\\mathbf{X} \\mathbf{w}_o + \\boldsymbol{\\eta}\n",
    "$$\n",
    "\n",
    " with $\\mathbf{y},\\boldsymbol{\\eta}\\in\\mathbb{R}^n$,$\\mathbf{w}_o\\in\n",
    "\\mathbb{R}^d$ and $\\mathbf{X}$ contains $\\mathbf{x}_i$ as columns.  The\n",
    "hypothesis set consists of all linear models,\n",
    "\n",
    "$$\n",
    "h(\\mathbf{w},\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x}\n",
    "$$\n",
    "\n",
    " We need to the learn the correct $\\mathbf{w}$ from the\n",
    "hypothesis set given the training data. So far, this is the usual\n",
    "setup for the problem, but how does the noise factor play to this? In\n",
    "our usual situation, the training set consists of randomly chosen\n",
    "elements from a larger space. In this case, that would be the same as\n",
    "getting random sets of $\\mathbf{x}_i$ vectors. That still happens in\n",
    "this case, but the problem is that even if the same $\\mathbf{x}_i$\n",
    "appears twice, it will not be associated with the same $y$ value due\n",
    "the additive noise coming from $\\eta$. To keep this simple, we assume\n",
    "that there is a fixed set of $\\mathbf{x}_i$ vectors and that we get\n",
    "all of them in the training set.  For every specific training set, we\n",
    "know how to solve for the MMSE from our earlier statistics work,\n",
    "\n",
    "$$\n",
    "\\mathbf{w} =  (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "Given this setup, what is the in-sample mean-squared-error? Because\n",
    "this is the MMSE solution, we know from our study of the associated\n",
    "orthogonality of such systems that we have,\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:Ein\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E_{\\texttt{in}}=\\lVert \\mathbf{y} \\rVert^2 - \\lVert \\mathbf{X w} \\rVert^2\n",
    "\\label{eq:Ein} \\tag{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    " where our best hypothesis, $\\mathbf{h} = \\mathbf{X w}$. Now, we want\n",
    "to compute the expectation of this over the distribution of $\\eta$.  For\n",
    "instance, for the first term, we want to compute,\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\lvert \\mathbf{y} \\rvert^2 = \\frac{1}{n} \\mathbb{E} (\\mathbf{y}^T\n",
    "\\mathbf{y}) = \\frac{1}{n} \\mathrm{Tr} \\: \\mathbb{E} (\\mathbf{y} \\mathbf{y}^T)\n",
    "$$\n",
    "\n",
    " where $\\mathrm{Tr}$ is the matrix trace operator (i.e., sum of the\n",
    "diagonal\n",
    "elements). Because each $\\eta$ are independent, we have\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:eyy\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathrm{Tr} \\: \\mathbb{E} (\\mathbf{y} \\mathbf{y}^T) = \\mathrm{Tr} \\: \\mathbf{X}\n",
    "\\mathbf{w}_o \\mathbf{w}_o^T \\mathbf{X}^T + \\sigma^2 \\mathrm{Tr} \\: \\mathbf{I} =\n",
    "\\mathrm{Tr} \\: \\mathbf{X} \\mathbf{w}_o \\mathbf{w}_o^T \\mathbf{X}^T + n \\sigma^2\n",
    "\\label{eq:eyy} \\tag{3}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    " where $\\mathbf{I}$ is the $n \\times n$ identity matrix. For the\n",
    "second term in Equation \\eq[eq:Ein](#eq:Ein), we have\n",
    "\n",
    "$$\n",
    "\\lvert\\mathbf{X w} \\rvert^2 = \\mathrm{Tr} \\: \\mathbf{X w}\\mathbf{w}^T\n",
    "\\mathbf{X}^T = \\mathrm{Tr}\\: \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1}\n",
    "\\mathbf{X}^T \\mathbf{y} \\mathbf{y}^T \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1}\n",
    "\\mathbf{X}^T\n",
    "$$\n",
    "\n",
    " The expectation of this is the following,\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{E} \\lvert \\mathbf{X w} \\rvert^2 = \\mathrm{Tr} \\:  \\mathbf{X}\n",
    "(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbb{E}(\\mathbf{y} \\mathbf{y}^T)\n",
    "\\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T\n",
    "\\label{_auto1} \\tag{4}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    " which, after substituting in Equation [eq:eyy](#eq:eyy), yields,\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{E} \\lvert \\mathbf{X w} \\rvert^2 = \\mathrm{Tr} \\:  \\mathbf{X}\n",
    "\\mathbf{w}_o \\mathbf{w}_o^T \\mathbf{X}^T + \\sigma^2 d\n",
    "\\label{_auto2} \\tag{5}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    " Next, assembling Equation [eq:Ein](#eq:Ein) from this and Equation\n",
    "[eq:eyy](#eq:eyy) gives,\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{E}(E_{\\texttt{in}})=\\frac{1}{n}E_{\\texttt{in}}=\\sigma^2\n",
    "\\left(1-\\frac{d}{n}\\right)\n",
    "\\label{_auto3} \\tag{6}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    " which provides an explicit relationship between the noise power,\n",
    "$\\sigma^2$, the complexity of the method ($d$) and the number of training\n",
    "samples ($n$).  This is very illustrative because it reveals the ratio $d/n$,\n",
    "which is a statement of the trade-off between model complexity and in-sample\n",
    "data size. From our analysis of the VC-dimension, we already know that there is\n",
    "a complicated bound that represents the penalty for complexity, but this\n",
    "problem is unusual in that we can actually derive an expression for this\n",
    "without resorting to bounding arguments. Furthermore, this result shows, that\n",
    "with a very large number of training examples ($n \\rightarrow \\infty$), the\n",
    "expected in-sample error approaches $\\sigma^2$. Informally, this means that the\n",
    "learning method cannot *generalize* from noise and thus can only reduce the\n",
    "expected in-sample error by memorizing the data (i.e., $d \\approx n$).\n",
    "\n",
    "The corresponding analysis for the expected out-of-sample error is similar,\n",
    "but more complicated because we don't have the orthogonality condition. Also,\n",
    "the out-of-sample data has different noise from that used to derive the weights,\n",
    "$\\mathbf{w}$. This results in extra cross-terms,\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E_{\\texttt{out}} = \\mathrm{Tr} \\Biggl( \\mathbf{X} \\mathbf{w}_o \\mathbf{w}_o^T\n",
    "\\mathbf{X}^T + \\boldsymbol{\\xi} \\boldsymbol{\\xi}^T + \\mathbf{X} \\mathbf{w}\n",
    "\\mathbf{w}^T \\mathbf{X}^T - \\mathbf{X} \\mathbf{w} \\mathbf{w}_o^T \\mathbf{X}^T\n",
    "\\notag\n",
    "\\label{_auto4} \\tag{7}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto5\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\\n",
    "                   - \\mathbf{X} \\mathbf{w}_o \\mathbf{w}^T \\mathbf{X}^T \\Biggr)\n",
    "\\label{_auto5} \\tag{8}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    " where we are using the $\\boldsymbol{\\xi}$ notation for the noise in\n",
    "the\n",
    "out-of-sample case, which is different from that in the in-sample case.\n",
    "Simplifying this leads to the following,\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto6\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{E}(E_{\\texttt{out}})=\\mathrm{Tr}\\: \\sigma^2 \\mathbf{I}  + \\sigma^2\n",
    "\\mathbf{X}(\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T\n",
    "\\label{_auto6} \\tag{9}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    " Then, assembling all of this gives,\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:Eout\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{E}(E_{\\texttt{out}}) = \\sigma^2 \\left(1+\\frac{d}{n}\\right)\n",
    "\\label{eq:Eout} \\tag{10}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    " which shows that even in the limit of large $n$, the expected\n",
    "out-of-sample error also approaches the noise power limit, $\\sigma^2$. This\n",
    "shows that memorizing the in-sample data (i.e., $d/n \\approx 1$) imposes a\n",
    "proportionate penalty on the out-of-sample performance (i.e., $\\mathbb{E}\n",
    "E_{\\texttt{out}} \\approx 2\\sigma^2$ when $\\mathbb{E}E_{\\texttt{in}} \\approx 0$\n",
    ").\n",
    "\n",
    "The following code simulates this important example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "40"
    }
   },
   "outputs": [],
   "source": [
    "def est_errors(d=3,n=10,niter=100):\n",
    "    assert n>d\n",
    "    wo = np.matrix(arange(d)).T \n",
    "    Ein = list()\n",
    "    Eout = list()\n",
    "    # choose any set of vectors\n",
    "    X = np.matrix(np.random.rand(n,d)) \n",
    "    for ni in range(niter):\n",
    "        y = X*wo + np.random.randn(X.shape[0],1)\n",
    "        # training weights\n",
    "        w = np.linalg.inv(X.T*X)*X.T*y\n",
    "        h = X*w\n",
    "        Ein.append(np.linalg.norm(h-y)**2)\n",
    "        # out of sample error\n",
    "        yp = X*wo + np.random.randn(X.shape[0],1)\n",
    "        Eout.append(np.linalg.norm(h-yp)**2)\n",
    "    return (np.mean(Ein)/n,np.mean(Eout)/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming Tip.**\n",
    "\n",
    "Python has an `assert` statement to make sure that certain entry conditions for\n",
    "the variables in the function are satisfied.  It is a good practice to use\n",
    "reasonable assertions at entry and exit to improve the quality of code.\n",
    "\n",
    "\n",
    "\n",
    " The following runs the simulation for the given value of $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "41"
    }
   },
   "outputs": [],
   "source": [
    "d=10\n",
    "xi = arange(d*2,d*10,d//2)\n",
    "ei,eo=np.array([est_errors(d=d,n=n,niter=100) for n in xi]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which results in [Figure](#fig:learning_theory_008). This\n",
    "figure shows the estimated expected in-sample and out-of-sample errors\n",
    "from our simulation compared with our  corresponding analytical\n",
    "result. The heavy horizontal line shows the variance of the additive\n",
    "noise $\\sigma^2=1$. Both these curves approach this asymptote because\n",
    "the noise is the ultimate learning limit for this problem. For a given\n",
    "dimension $d$, even with an infinite amount of training data, the\n",
    "learning method cannot generalize beyond the limit of the noise power.\n",
    "Thus, the expected generalization error is\n",
    "$\\mathbb{E}(E_{\\texttt{out}})-\\mathbb{E}(E_{\\texttt{in}})=2\\sigma^2\\frac{d}{n}$.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-machine_learning/learning_theory_008.png, width=500\n",
    "frac=0.85]  The dots show the learning curves estimated from the simulation and\n",
    "the solid lines show the corresponding terms for our analytical result. The\n",
    "horizontal line shows the variance of the additive noise ($\\sigma^2=1$ in this\n",
    "case). Both the expected in-sample and out-of-sample errors asymptotically\n",
    "approach this line. <div id=\"fig:learning_theory_008\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:learning_theory_008\"></div>\n",
    "\n",
    "<p>The dots show the learning curves estimated from the simulation and the solid\n",
    "lines show the corresponding terms for our analytical result. The horizontal\n",
    "line shows the variance of the additive noise ($\\sigma^2=1$ in this case). Both\n",
    "the expected in-sample and out-of-sample errors asymptotically approach this\n",
    "line.</p>\n",
    "<img src=\"fig-machine_learning/learning_theory_008.png\" width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "42"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax=subplots()\n",
    "fig.set_size_inches((10,6))\n",
    "_=ax.plot(xi,ei,'ks',label=r'$\\hat{E}_{in}$',lw=3.,ms=10,alpha=.5)\n",
    "_=ax.plot(xi,eo,'ko',label=r'$\\hat{E}_{out}$',lw=3.,ms=10,alpha=.5)\n",
    "_=ax.plot(xi,(1-d/np.array(xi)),'--k',label=r'${E}_{in}$',lw=3)\n",
    "_=ax.plot(xi,(1+d/np.array(xi)),':k',label=r'${E}_{out}$')\n",
    "_=ax.hlines(1,xi.min(),xi.max(),lw=3)\n",
    "_=ax.set_xlabel('size of training set ($n$)',fontsize=22)\n",
    "_=ax.set_ylabel('MSE',fontsize=22)\n",
    "_=ax.legend(loc=4,fontsize=18)\n",
    "_=ax.set_title('dimension = %d'%(d),fontsize=22)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "43"
    }
   },
   "outputs": [],
   "source": [
    "sys.displayhook= old_displayhook"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
